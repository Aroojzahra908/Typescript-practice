{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install unsloth\n!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-09T19:00:22.427685Z","iopub.execute_input":"2025-03-09T19:00:22.428036Z","iopub.status.idle":"2025-03-09T19:00:42.620391Z","shell.execute_reply.started":"2025-03-09T19:00:22.428004Z","shell.execute_reply":"2025-03-09T19:00:42.619290Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\n# Define model parameters\nmax_seq_length = 2048  # Adjust according to your dataset\ndtype = None  # Auto-detect dtype\nload_in_4bit = True  # Use 4-bit quantization to save VRAM\n\n# Load Llama 3.2 3B Instruct model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-3B-Instruct\", \n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T19:00:47.826543Z","iopub.execute_input":"2025-03-09T19:00:47.826853Z","iopub.status.idle":"2025-03-09T19:01:36.119251Z","shell.execute_reply.started":"2025-03-09T19:00:47.826822Z","shell.execute_reply":"2025-03-09T19:01:36.118168Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.3.9: Fast Llama patching. Transformers: 4.49.0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.35G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fa762b27b3140a2ba4836a846094a83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81619dbfe0d34ad7871e40ad9809ffc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b000c8500304bdaad6f33b1a183076f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"252c9ad6f3584579b8656ee91aa806a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1b19105776349beba449254d001fc93"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,  # Use 16 for LoRA rank (adjust as needed)\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    lora_dropout = 0,  # No dropout to optimize\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    use_rslora = False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T19:01:57.423993Z","iopub.execute_input":"2025-03-09T19:01:57.424437Z","iopub.status.idle":"2025-03-09T19:02:04.211264Z","shell.execute_reply.started":"2025-03-09T19:01:57.424408Z","shell.execute_reply":"2025-03-09T19:02:04.210156Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.3.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pdfplumber\n\ndef extract_text_from_pdf(pdf_path):\n    text = \"\"\n    with pdfplumber.open(pdf_path) as pdf:\n        for page in pdf.pages:\n            text += page.extract_text()\n    return text\n\n# Example usage\npdf_path = \"path_to_your_pdf_file.pdf\"\nextracted_text = extract_text_from_pdf(pdf_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_text(text):\n    # Example: Remove unwanted characters and normalize spaces\n    cleaned_text = text.replace(\"\\n\", \" \").strip()\n    # Additional preprocessing steps can go here\n    return cleaned_text\n\npreprocessed_text = preprocess_text(extracted_text)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Assuming you have your own dataset stored as a list of examples:\ndataset = load_dataset('json', data_files='your_dataset_file.json', split='train')\n\ndef format_dataset_for_training(examples):\n    texts = examples[\"extracted_texts\"]  # Assuming your data has \"extracted_texts\" as the key\n    formatted_texts = [tokenizer.apply_chat_template(text, tokenize=False, add_generation_prompt=False) for text in texts]\n    return {\"text\": formatted_texts}\n\ndataset = dataset.map(format_dataset_for_training, batched=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_response_completeness(response):\n    # Example: Check if the response ends with a proper punctuation mark or full sentence\n    if response.strip()[-1] not in [\".\", \"!\", \"?\"]:\n        return False  # If incomplete, return False\n    return True\n\n# After generating the response from the model\ngenerated_response = tokenizer.batch_decode(outputs)[0]\n\nif not check_response_completeness(generated_response):\n    print(\"Incomplete response. Please ask again.\")\nelse:\n    print(\"Complete response:\", generated_response)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Assuming your dataset is in ShareGPT format or needs to be formatted to this style\ndataset = load_dataset(\"path_to_your_dataset\", split=\"train\")\n\nfrom unsloth.chat_templates import standardize_sharegpt\n\n# Standardize dataset into the appropriate format\ndataset = standardize_sharegpt(dataset)\n\n# Apply any additional formatting if needed\ndef formatting_prompts_func(examples):\n    convos = examples[\"conversations\"]\n    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n    return {\"text\": texts}\n\ndataset = dataset.map(formatting_prompts_func, batched=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n)\n\ndef formatting_prompts_func(examples):\n    convos = examples[\"conversations\"]\n    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import standardize_sharegpt\ndataset = standardize_sharegpt(dataset)\ndataset = dataset.map(formatting_prompts_func, batched = True,)\n\ndataset[5][\"conversations\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\ntokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments, DataCollatorForSeq2Seq\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n    dataset_num_proc=2,\n    packing=False,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_steps=5,\n        max_steps=60,  # Set a smaller number of steps for faster results\n        learning_rate=2e-4,\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\"\n    ),\n)\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import train_on_responses_only\n\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare the model for inference\nFastLanguageModel.for_inference(model)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of Algeria?\"}\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n).to(\"cuda\")\n\noutputs = model.generate(input_ids=inputs, max_new_tokens=64, use_cache=True, temperature=1.5, min_p=0.1)\ntokenizer.batch_decode(outputs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"lora_model\")  # Save locally\ntokenizer.save_pretrained(\"lora_model\")\n\n# Optionally push to the Hugging Face hub\n# model.push_to_hub(\"your_name/lora_model\", token=\"...\")\n# tokenizer.push_to_hub(\"your_name/lora_model\", token=\"...\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained_gguf(\"model_3B\", tokenizer, quantization_method=\"f16\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}