{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12180/1243437880.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow.keras as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = 785\n",
    "\n",
    "# Create column names\n",
    "columns = ['label'] + list(range(1, num_columns))\n",
    "\n",
    "# Read the training and testing datasets\n",
    "ds_train = pd.read_csv(\"emnist-balanced-train.csv\", header=None)\n",
    "ds_test = pd.read_csv(\"emnist-balanced-test.csv\", header=None)\n",
    "\n",
    "# Assign column names to the DataFrames\n",
    "ds_train.columns = columns\n",
    "ds_test.columns = columns\n",
    "\n",
    "# Reset index\n",
    "ds_train = ds_train.reset_index(drop=True)\n",
    "ds_test = ds_test.reset_index(drop=True)\n",
    "\n",
    "# Extract features and labels\n",
    "x_train = ds_train.drop(['label'], axis=1)\n",
    "y_train = ds_train['label']\n",
    "x_test = ds_test.drop(['label'], axis=1)\n",
    "y_test = ds_test['label']\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(ds_train.head())\n",
    "print(ds_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  62 classes , mapped to actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_num_to_char = {\n",
    "    0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9',\n",
    "    10: 'a', 11: 'b', 12: 'c', 13: 'd', 14: 'e', 15: 'f', 16: 'g', 17: 'h', 18: 'i', 19: 'j',\n",
    "    20: 'k', 21: 'l', 22: 'm', 23: 'n', 24: 'o', 25: 'p', 26: 'q', 27: 'r', 28: 's', 29: 't',\n",
    "    30: 'u', 31: 'v', 32: 'w', 33: 'x', 34: 'y', 35: 'z',\n",
    "    36: 'A', 37: 'B', 38: 'C', 39: 'D', 40: 'E', 41: 'F', 42: 'G', 43: 'H', 44: 'I', 45: 'J',\n",
    "    46: 'K', 47: 'L', 48: 'M', 49: 'N', 50: 'O', 51: 'P', 52: 'Q', 53: 'R', 54: 'S', 55: 'T',\n",
    "    56: 'U', 57: 'V', 58: 'W', 59: 'X', 60: 'Y', 61: 'Z'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example(n) :\n",
    "    label = alpha_num_to_char[ds_train.iloc[n,0]]  # the example's label\n",
    "    plt.title(label)\n",
    "    example = np.array(x_train.iloc[n]) # extract the 28 *28 array that contains the pixels\n",
    "    example_image = example.reshape(28 , 28 ) # rearrange the array to be a 28 by 28 matrix ,that represents that actual image\n",
    "    plt.imshow(example_image , cmap ='gray')\n",
    "    plt.axis('off')\n",
    "  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example_and_predicted_val(n,predicted_n) :\n",
    "\n",
    "    true_val = alpha_num_to_char[ds_train.iloc[n,0]]  # the example's label\n",
    "    predicted_val = alpha_num_to_char[predicted_n]  # the example's label\n",
    "    \n",
    "    plt.title(\"Actual val : \"+ true_val+\"\\n Predicted :\" + predicted_val)\n",
    "    example = np.array(x_train.iloc[n]) # extract the 28 *28 array that contains the pixels\n",
    "    example_image = example.reshape(28 , 28 ) # rearrange the array to be a 28 by 28 matrix ,that represents that actual image\n",
    "    plt.imshow(example_image , cmap ='gray')\n",
    "    plt.axis('off')\n",
    "  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_example(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing using a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class Reshaper(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_reshaped = X.reshape(-1, 28, 28, 1)\n",
    "        return X_reshaped\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Step 1: StandardScaler\n",
    "    ('reshaper', Reshaper())  # Step 2: Custom Reshaper\n",
    "])\n",
    "\n",
    "\n",
    "# Fit and transform the training data\n",
    "\n",
    "x_train_processed = pipeline.fit_transform(x_train)\n",
    "y_train_processed = np.eye(62)[y_train.astype(int)]\n",
    "\n",
    "x_test_processed = pipeline.transform(x_test)\n",
    "\n",
    "y_test_processed = np.eye(62)[y_test.astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('input shape',x_train_processed.shape)\n",
    "print('output shape',y_train.shape)\n",
    "# y_train = np.eye(62)[y_train.astype(int)]\n",
    "print('output shape',y_train_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D,Dropout\n",
    "\n",
    "#  multiple model architectures\n",
    "models = []\n",
    "input_shape=(28,28,1)\n",
    "# Model 1\n",
    "model1 = Sequential()\n",
    "model1.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model1.add(MaxPooling2D((2, 2)))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "model1.add(Dense(62, activation='softmax'))\n",
    "models.append(model1)\n",
    "\n",
    "# Model 2\n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model2.add(Dropout(0.25)) \n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dropout(0.4))\n",
    "model2.add(Dense(62, activation='softmax'))\n",
    "# Add layers for Model 2\n",
    "models.append(model2)\n",
    "\n",
    "# Model 3\n",
    "model3 = Sequential()\n",
    "model3.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model3.add(Dropout(0.25)) \n",
    "model3.add(MaxPooling2D((2, 2)))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(256, activation='relu'))\n",
    "model3.add(Dropout(0.4))\n",
    "model3.add(Dense(128, activation='relu'))\n",
    "model3.add(Dense(62, activation='softmax'))\n",
    "models.append(model3)\n",
    "\n",
    "# Train each model\n",
    "for i, model in enumerate(models):\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(x_train_processed, y_train_processed, epochs=5, batch_size=32)\n",
    "\n",
    "# Evaluate each model\n",
    "accuracies = []\n",
    "for i, model in enumerate(models):\n",
    "    loss, accuracy = model.evaluate(x_test_processed, y_test_processed)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Model {i+1} Accuracy: {accuracy}\")\n",
    "\n",
    "# Select the best model based on accuracy\n",
    "best_model_index = accuracies.index(max(accuracies))\n",
    "best_model = models[best_model_index]\n",
    "print(f\"Best Model: Model {best_model_index+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some example from the data set\n",
    "x=65765\n",
    "example0=np.argmax(best_model.predict(x_train_processed[x].reshape(1,28,28,1)).flatten())\n",
    "alpha_num_to_char[example0]\n",
    "show_example_and_predicted_val(x,example0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(df,open('df.pkl','wb'))\n",
    "# pickle.dump(pipe,open('pipe.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('best_model.h5')\n",
    "\n",
    "# Now you can use the loaded_model for inference or further training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# some example from the data set\n",
    "x=68785\n",
    "example0=np.argmax(loaded_model.predict(x_train_processed[x].reshape(1,28,28,1)).flatten())\n",
    "alpha_num_to_char[example0]\n",
    "show_example_and_predicted_val(x,example0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
