{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch transformers accelerate bitsandbytes peft pdfplumber datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "def extract_text_from_pdfs(pdf_folder):\n",
    "    all_texts = []\n",
    "    for pdf_file in os.listdir(pdf_folder):\n",
    "        if pdf_file.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                text = \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "                all_texts.append(text)\n",
    "    return all_texts\n",
    "\n",
    "# Set your folder path\n",
    "pdf_folder = \"path/to/your/pdf/folder\"\n",
    "dataset_texts = extract_text_from_pdfs(pdf_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def format_dataset(texts):\n",
    "    formatted_data = []\n",
    "    for text in texts:\n",
    "        formatted_data.append({\"prompt\": \"Summarize this document:\", \"response\": text[:1000]})  # Truncate long texts\n",
    "    return Dataset.from_dict({\"prompt\": [d[\"prompt\"] for d in formatted_data], \"response\": [d[\"response\"] for d in formatted_data]})\n",
    "\n",
    "dataset = format_dataset(dataset_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load Llama 3 base model\n",
    "model_name = \"meta-llama/Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"auto\", torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8, lora_alpha=32, lora_dropout=0.1, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"prompt\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=tokenized_dataset, tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install llama-cpp-python\n",
    "# python convert.py --model ./llama3_finetuned --output llama3.gguf --quantize q4_0\n",
    "# run on bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"./llama3.gguf\")\n",
    "response = llm(\"Summarize this document: ...\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from llama_cpp import Llama\n",
    "\n",
    "app = FastAPI()\n",
    "llm = Llama(model_path=\"./llama3.gguf\")\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate(prompt: str):\n",
    "    response = llm(prompt, max_tokens=100)\n",
    "    return {\"response\": response[\"choices\"][0][\"text\"]}\n",
    "\n",
    "# Run server: uvicorn filename:app --reload\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
